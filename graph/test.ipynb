{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def is_local_module(module_name: str, code_dir: str) -> bool:\n",
    "    module_name = module_name.replace(\"code_base.\", \"\")  # ‚úÖ Strip project root\n",
    "    rel_path = module_name.replace(\".\", os.sep) + \".py\"\n",
    "    module_path = os.path.join(code_dir, rel_path)\n",
    "    return os.path.isfile(module_path)\n",
    "\n",
    "\n",
    "def make_node_id(filename: str, func: str) -> str:\n",
    "    \"\"\"Standardized graph node ID: <filename>:<function>\"\"\"\n",
    "    return f\"{os.path.basename(filename)}:{func}\"\n",
    "\n",
    "def strip_base_module_prefix(module_name: str, code_dir: str):\n",
    "    abs_code_dir = os.path.abspath(code_dir)\n",
    "    for root, _, files in os.walk(abs_code_dir):\n",
    "        for f in files:\n",
    "            if f.endswith(\".py\"):\n",
    "                module_guess = os.path.splitext(f)[0]\n",
    "                if module_guess == module_name or module_guess.endswith(f\".{module_name}\"):\n",
    "                    return module_guess\n",
    "    return module_name\n",
    "\n",
    "\n",
    "\n",
    "class CodeAnalyzer(ast.NodeVisitor):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.imports = {}  # alias -> module:function or module\n",
    "        self.func_defs = {}  # function name -> FunctionDef\n",
    "        self.func_calls = defaultdict(list)  # function -> list of called names\n",
    "        self.var_usage = defaultdict(set)  # function -> set of variable names\n",
    "\n",
    "    def visit_ImportFrom(self, node):\n",
    "        module = node.module\n",
    "        for alias in node.names:\n",
    "            self.imports[alias.asname or alias.name] = f\"{module}:{alias.name}\"\n",
    "\n",
    "    def visit_Import(self, node):\n",
    "        for alias in node.names:\n",
    "            self.imports[alias.asname or alias.name] = alias.name\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        func_name = node.name\n",
    "        self.func_defs[func_name] = node\n",
    "        for inner in ast.walk(node):\n",
    "            if isinstance(inner, ast.Call):\n",
    "                if isinstance(inner.func, ast.Name):\n",
    "                    self.func_calls[func_name].append(inner.func.id)\n",
    "                elif isinstance(inner.func, ast.Attribute):\n",
    "                    self.func_calls[func_name].append(inner.func.attr)\n",
    "            elif isinstance(inner, ast.Name):\n",
    "                self.var_usage[func_name].add(inner.id)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "\n",
    "def build_code_graph(code_dir: str) -> nx.DiGraph:\n",
    "    graph = nx.DiGraph()\n",
    "    code_dir = os.path.abspath(code_dir)\n",
    "    all_py_files = [\n",
    "        os.path.join(code_dir, f)\n",
    "        for f in os.listdir(code_dir)\n",
    "        if f.endswith(\".py\")\n",
    "    ]\n",
    "\n",
    "    analyzers = {}\n",
    "    for file_path in all_py_files:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            tree = ast.parse(f.read(), filename=file_path)\n",
    "        analyzer = CodeAnalyzer(file_path)\n",
    "        analyzer.visit(tree)\n",
    "        analyzers[file_path] = analyzer\n",
    "\n",
    "    for file_path, analyzer in analyzers.items():\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        # Add function nodes\n",
    "        for func_name in analyzer.func_defs:\n",
    "            node_id = make_node_id(file_name, func_name)\n",
    "            graph.add_node(\n",
    "                node_id,\n",
    "                file=file_name,\n",
    "                func=func_name,\n",
    "                variables=list(analyzer.var_usage.get(func_name, [])),\n",
    "                external=False,\n",
    "            )\n",
    "\n",
    "        # Add edges for function calls\n",
    "        for caller, callees in analyzer.func_calls.items():\n",
    "            caller_node = make_node_id(file_name, caller)\n",
    "            for callee in callees:\n",
    "                callee_node = None\n",
    "                edge_type = \"calls\"\n",
    "\n",
    "                imported = analyzer.imports.get(callee)\n",
    "                if imported and \":\" in imported:\n",
    "                    mod, func = imported.split(\":\")\n",
    "                    mod = strip_base_module_prefix(mod, code_dir)\n",
    "                    if is_local_module(mod, code_dir):\n",
    "                        callee_node = make_node_id(f\"{mod}.py\", func)\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Warning: `{callee}` imported from `{mod}`, not found locally. Marking as external.\")\n",
    "                        callee_node = f\"<external>:{func}\"\n",
    "                        edge_type = \"calls_external\"\n",
    "                        graph.add_node(callee_node, func=func, file=\"<external>\", external=True)\n",
    "                elif imported:\n",
    "                    # e.g., imported = \"numpy\"\n",
    "                    callee_node = f\"<external>:{callee}\"\n",
    "                    edge_type = \"calls_external\"\n",
    "                    graph.add_node(callee_node, func=callee, file=\"<external>\", external=True)\n",
    "                else:\n",
    "                    # Assume it's a local call in the same file\n",
    "                    callee_node = make_node_id(file_name, callee)\n",
    "\n",
    "                if callee_node:\n",
    "                    if not graph.has_node(callee_node):\n",
    "                        graph.add_node(\n",
    "                            callee_node,\n",
    "                            func=callee,\n",
    "                            file=callee_node.split(\":\")[0],\n",
    "                            external=callee_node.startswith(\"<external>\"),\n",
    "                        )\n",
    "                    graph.add_edge(caller_node, callee_node, type=edge_type)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def explain_variable_relation(graph: nx.DiGraph, var1: str, var2: str):\n",
    "    nodes_var1 = [n for n, d in graph.nodes(data=True) if var1 in d.get(\"variables\", [])]\n",
    "    nodes_var2 = [n for n, d in graph.nodes(data=True) if var2 in d.get(\"variables\", [])]\n",
    "\n",
    "    if not nodes_var1:\n",
    "        print(f\"‚ùå Variable `{var1}` not found in any function.\")\n",
    "    if not nodes_var2:\n",
    "        print(f\"‚ùå Variable `{var2}` not found in any function.\")\n",
    "    if not nodes_var1 or not nodes_var2:\n",
    "        return\n",
    "\n",
    "    print(f\"\\nüîç `{var1}` found in: {nodes_var1}\")\n",
    "    print(f\"üîç `{var2}` found in: {nodes_var2}\")\n",
    "\n",
    "    found_path = False\n",
    "    for src in nodes_var1:\n",
    "        for tgt in nodes_var2:\n",
    "            if nx.has_path(graph, src, tgt):\n",
    "                path = nx.shortest_path(graph, src, tgt)\n",
    "                print(f\"\\nüìà Relationship path from `{var1}` to `{var2}`:\")\n",
    "                for i in range(len(path) - 1):\n",
    "                    a, b = path[i], path[i + 1]\n",
    "                    etype = graph.edges[a, b].get(\"type\", \"\")\n",
    "                    print(f\"  {a} --[{etype}]--> {b}\")\n",
    "                found_path = True\n",
    "\n",
    "    if not found_path:\n",
    "        print(\"\\n‚ùó No path found between the functions using the two variables.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Nodes:\n",
      "a.py:add -> {'file': 'a.py', 'func': 'add', 'variables': ['y', 'x'], 'external': False}\n",
      "c.py:display -> {'file': 'c.py', 'func': 'display', 'variables': ['print', 'val'], 'external': False}\n",
      "c.py:print -> {'func': 'print', 'file': 'c.py', 'external': False}\n",
      "b.py:calc -> {'file': 'b.py', 'func': 'calc', 'variables': ['result', 'add', 'a', 'b'], 'external': False}\n",
      "code_base.a.py:add -> {'func': 'add', 'file': 'code_base.a.py', 'external': False}\n",
      "\n",
      "üîÅ Edges:\n",
      "c.py:display --[calls]--> c.py:print\n",
      "b.py:calc --[calls]--> code_base.a.py:add\n",
      "\n",
      "üîç `x` found in: ['a.py:add']\n",
      "üîç `result` found in: ['b.py:calc']\n",
      "\n",
      "‚ùó No path found between the functions using the two variables.\n"
     ]
    }
   ],
   "source": [
    "code_dir = \"./code_base\"  # Your code folder\n",
    "graph = build_code_graph(code_dir)\n",
    "\n",
    "# Show graph structure\n",
    "print(\"\\nüì¶ Nodes:\")\n",
    "for n, d in graph.nodes(data=True):\n",
    "    print(f\"{n} -> {d}\")\n",
    "\n",
    "print(\"\\nüîÅ Edges:\")\n",
    "for u, v, d in graph.edges(data=True):\n",
    "    print(f\"{u} --[{d['type']}]--> {v}\")\n",
    "\n",
    "# Variable trace across the graph\n",
    "explain_variable_relation(graph, \"x\", \"result\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn numpy sentence-transformers\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Example code chunks (function-level or file snippets)\n",
    "code_chunks = [\n",
    "    \"def load_config(path):\\n    with open(path) as f:\\n        return json.load(f)\",\n",
    "    \"config = load_config('settings.json')\",\n",
    "    \"def process_data(data):\\n    return [d for d in data if d is not None]\",\n",
    "    \"# This function processes the user data and removes null entries\",\n",
    "    \"def main():\\n    data = fetch_data()\\n    clean_data = process_data(data)\"\n",
    "]\n",
    "\n",
    "# Your input query\n",
    "query = \"explain the variable config\"\n",
    "\n",
    "# ---------------------\n",
    "# 1. Sparse BM25-style retrieval\n",
    "# ---------------------\n",
    "vectorizer = TfidfVectorizer()\n",
    "sparse_matrix = vectorizer.fit_transform(code_chunks)\n",
    "sparse_query_vec = vectorizer.transform([query])\n",
    "bm25_scores = cosine_similarity(sparse_query_vec, sparse_matrix)[0]  # cosine ~ BM25 for short texts\n",
    "\n",
    "# ---------------------\n",
    "# 2. Dense embedding retrieval\n",
    "# ---------------------\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # You can replace with CodeBERT or CodeT5 for better code results\n",
    "chunk_embeddings = model.encode(code_chunks, convert_to_tensor=False)\n",
    "query_embedding = model.encode([query], convert_to_tensor=False)[0]\n",
    "dense_scores = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "\n",
    "# ---------------------\n",
    "# 3. Score Fusion (normalize + blend)\n",
    "# ---------------------\n",
    "bm25_norm = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + 1e-8)\n",
    "dense_norm = (dense_scores - np.min(dense_scores)) / (np.max(dense_scores) - np.min(dense_scores) + 1e-8)\n",
    "\n",
    "alpha = 0.5  # weight between dense and sparse\n",
    "hybrid_scores = alpha * dense_norm + (1 - alpha) * bm25_norm\n",
    "\n",
    "# ---------------------\n",
    "# 4. Get Top-K Chunks\n",
    "# ---------------------\n",
    "top_k = 3\n",
    "top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "for i in top_indices:\n",
    "    print(f\"Score: {hybrid_scores[i]:.4f}\")\n",
    "    print(code_chunks[i])\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
